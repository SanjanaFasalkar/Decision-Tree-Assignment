{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N2GnhtjOB31"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question\n",
        "\n",
        "1. What is a Decision Tree, and how does it work in the context of\n",
        "  classification?\n",
        " -    answers:- A Decision Tree is a flowchart-like tree structure where:\n",
        "\n",
        "-  Each internal node represents a feature (attribute) test\n",
        "\n",
        "-  Each branch represents the outcome of that test\n",
        "\n",
        "-  Each leaf node represents a class label (decision taken after computing all  features)\n",
        "\n",
        "-    How It Works (Classification Context)\n",
        "Start at the root node (the full dataset).\n",
        "\n",
        "-  Partition the data into subsets based on the selected feature.\n",
        "\n",
        "-  Repeat this process recursively for each child node (subtree) until one of the stopping conditions is met:\n",
        "\n",
        "-  All data points in a node belong to the same class\n",
        "\n",
        "-  Maximum depth is reached\n",
        "\n",
        "-  Minimum number of samples per leaf is reached\n",
        "\n",
        "-  Splitting Criteria (How to Choose the Best Feature)?\n",
        "Common metrics for classification:\n",
        "\n",
        "-  Gini Impurity: Measures how often a randomly chosen element would be incorrectly labeled.\n",
        "\n",
        "-  Entropy / Information Gain: Measures the reduction in uncertainty after a split.\n",
        "\n",
        "-  Example (Binary Classification)\n",
        "Let‚Äôs say you want to classify whether a person buys a product based on age and income.\n",
        "\n",
        "        -  Age\tIncome\tBuys\n",
        "          <30\tHigh\tNo\n",
        "          30‚Äì40\tMedium\tYes\n",
        "         >40\tLow\tYes\n",
        "\n",
        "-  The tree might look like this:\n",
        "\n",
        "\n",
        "            [Age?]\n",
        "           /  |   \\\n",
        "        <30 30-40 >40\n",
        "        No   Yes   Yes\n",
        "\n",
        "-   No need for feature scaling\n",
        "\n",
        "-  Can handle both numerical and categorical data\n",
        "\n",
        "-  Prone to overfitting (especially deep trees)\n",
        "\n",
        "-  Small changes in data can result in very different trees (unstable)\n",
        "\n",
        "-  Biased towards features with more levels/categories\n",
        "\n",
        "-  Medical diagnosis (e.g., disease classification)\n",
        "\n",
        "-  Customer segmentation\n",
        "\n",
        "-  Credit scoring\n",
        "\n",
        "\n",
        " Question\n",
        "\n",
        " 2: Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "  How do they impact the splits in a Decision Tree?\n",
        "-  answers:- In decision tree algorithms like CART and ID3, we use impurity measures to decide which attribute to split on at each node. Two commonly used measures of impurity are:\n",
        "\n",
        "-  1. Gini Impurity\n",
        "-  Definition:\n",
        "Gini impurity measures the probability of incorrectly classifying a randomly chosen element if it was randomly labeled according to the distribution of labels in the dataset.\n",
        "\n",
        "-  Formula:\n",
        "\n",
        "-  ùê∫\n",
        "ùëñ\n",
        "ùëõ\n",
        "ùëñ\n",
        "(\n",
        "ùê∑\n",
        ")\n",
        "=\n",
        "1\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùê∂\n",
        "ùëù\n",
        "ùëñ\n",
        "2\n",
        "Gini(D)=1‚àí\n",
        "i=1\n",
        "‚àë\n",
        "C\n",
        "‚Äã\n",
        " p\n",
        "i\n",
        "2\n",
        "‚Äã\n",
        "\n",
        "Where:\n",
        "\n",
        "ùê∑\n",
        "D is the dataset\n",
        "\n",
        "ùê∂\n",
        "C is the number of classes\n",
        "\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "‚Äã\n",
        "  is the proportion of class\n",
        "ùëñ\n",
        "i in\n",
        "ùê∑\n",
        "D\n",
        "\n",
        "-  Interpretation:\n",
        "\n",
        "-  Range: 0 (pure) to just under 1 (impure)\n",
        "\n",
        "-  Lower Gini = better split (more homogeneous node)\n",
        "\n",
        "-  2. Entropy (Information Gain)\n",
        "-  Definition:\n",
        "-  Entropy measures the amount of disorder or uncertainty in the dataset. It is used in the ID3 algorithm.\n",
        "\n",
        "-  Formula:\n",
        "\n",
        "-  ùê∏\n",
        "ùëõ\n",
        "ùë°\n",
        "ùëü\n",
        "ùëú\n",
        "ùëù\n",
        "ùë¶\n",
        "(\n",
        "ùê∑\n",
        ")\n",
        "=\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùê∂\n",
        "ùëù\n",
        "ùëñ\n",
        "log\n",
        "‚Å°\n",
        "2\n",
        "(\n",
        "ùëù\n",
        "ùëñ\n",
        ")\n",
        "Entropy(D)=‚àí\n",
        "i=1\n",
        "‚àë\n",
        "C\n",
        "‚Äã\n",
        " p\n",
        "i\n",
        "‚Äã\n",
        " log\n",
        "2\n",
        "‚Äã\n",
        " (p\n",
        "i\n",
        "‚Äã\n",
        " )\n",
        "Where:\n",
        "\n",
        "ùëù\n",
        "ùëñ\n",
        "p\n",
        "i\n",
        "‚Äã\n",
        "  is the probability of class\n",
        "ùëñ\n",
        "i\n",
        "\n",
        "-  Interpretation:\n",
        "\n",
        "-  Entropy = 0 when the node is pure (only one class)\n",
        "\n",
        "- Entropy is highest when all classes are equally represented\n",
        "\n",
        "-  How They Impact Splits in a Decision Tree\n",
        "When building a decision tree:\n",
        "\n",
        "- At each node, we evaluate all possible splits of the data.\n",
        "\n",
        "-  For each split, we calculate the impurity (Gini or Entropy) of the resulting subsets.\n",
        "\n",
        "-  We aim to reduce impurity ‚Äî i.e., find the split that maximizes purity in the child nodes.\n",
        "\n",
        "-  The splitting criterion:\n",
        "Gini: Choose the split that gives the lowest weighted Gini impurity.\n",
        "\n",
        "-  Entropy (Information Gain): Choose the split that gives the highest information gain, i.e., maximum reduction in entropy.\n",
        "\n",
        "-   Example (Binary Classification):\n",
        "Class Distribution\tGini Impurity\tEntropy\n",
        "[50%, 50%]\t0.5\t1.0\n",
        "[90%, 10%]\t0.18\t0.47\n",
        "[100%, 0%]\t0.0 (pure)\t0.0 (pure)\n",
        "\n",
        "-   Gini vs. Entropy\n",
        "Criteria\tGini Impurity\tEntropy\n",
        "Algorithm Used\tCART\tID3, C4.5\n",
        "Computational Cost\tFaster (no logarithms)\tSlightly slower\n",
        "Splitting Behavior\tTends to isolate the most frequent class\tMore balanced splits\n",
        "\n",
        "\n",
        "Question\n",
        "\n",
        " 3: What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "  Trees? Give one practical advantage of using each.\n",
        "\n",
        "  -  answers:- Decision Trees can easily overfit the training data if allowed to grow without constraints. To prevent this, pruning techniques are used ‚Äî either before or after the tree is built.\n",
        "\n",
        "-   Pre-Pruning (Early Stopping)\n",
        "Definition:\n",
        "Pre-pruning stops the tree from growing once a certain condition is met during the training process.\n",
        "\n",
        "-  Common Pre-Pruning Criteria:\n",
        "\n",
        "-  Maximum depth of the tree\n",
        "\n",
        "-  Minimum number of samples required to split a node\n",
        "\n",
        "-  Minimum information gain or reduction in impurity (Gini/Entropy)\n",
        "\n",
        "-  Practical Advantage:\n",
        "Faster training time ‚Äî because it avoids creating unnecessary branches early, it reduces computation and saves resources.\n",
        "\n",
        "-  Post-Pruning (Cost Complexity Pruning or Reduced Error Pruning)\n",
        "Definition:\n",
        "Post-pruning allows the tree to grow fully and then removes branches that do not provide significant improvement on a validation set.\n",
        "\n",
        "-  How it works:\n",
        "\n",
        "-  Fully grow the tree\n",
        "\n",
        "-  Evaluate subtrees using a validation set\n",
        "\n",
        "-  Prune the branches that reduce accuracy or have minimal gain\n",
        "\n",
        "-  Practical Advantage:\n",
        "Better generalization ‚Äî the tree is allowed to learn all patterns and then simplified, often leading to higher accuracy on unseen data.\n",
        "\n",
        "-  Summary Table:\n",
        "Aspect\tPre-Pruning\tPost-Pruning\n",
        "When applied\tDuring tree construction\tAfter full tree is built\n",
        "Basis\tHeuristics (e.g. depth, samples)\tValidation performance\n",
        "Risk\tMight underfit\tLess risk of underfitting\n",
        "Speed\tFaster\tSlower\n",
        "Advantage\tFaster training\tBetter generalization\n",
        "\n",
        "\n",
        " Question\n",
        "\n",
        "  4: What is Information Gain in Decision Trees, and why is it important for\n",
        "  choosing the best split?\n",
        "-  answera:-\n",
        "-  Information Gain (IG) is a metric used in Decision Trees to measure the reduction in uncertainty (impurity) about the target variable after a dataset is split on a specific feature.\n",
        "\n",
        "-  It is based on the concept of Entropy, which measures the amount of disorder or impurity in a dataset.\n",
        "\n",
        "-  The formula for Information Gain is:\n",
        "\n",
        "-  Information¬†Gain\n",
        "=\n",
        "Entropy¬†(parent)\n",
        "‚àí\n",
        "‚àë\n",
        "ùëñ\n",
        "=\n",
        "1\n",
        "ùëò\n",
        "(\n",
        "ùëõ\n",
        "ùëñ\n",
        "ùëõ\n",
        "√ó\n",
        "Entropy¬†(child\n",
        "ùëñ\n",
        ")\n",
        ")\n",
        "Information¬†Gain=Entropy¬†(parent)‚àí\n",
        "i=1\n",
        "‚àë\n",
        "k\n",
        "‚Äã\n",
        " (\n",
        "n\n",
        "n\n",
        "i\n",
        "‚Äã\n",
        "\n",
        "‚Äã\n",
        "-   √óEntropy¬†(child\n",
        "i\n",
        "‚Äã\n",
        " ))\n",
        "Where:\n",
        "\n",
        "-  Entropy(parent): Entropy before the split.\n",
        "\n",
        "-  Entropy(child‚ÇÅ, child‚ÇÇ, ..., child‚Çñ): Entropies of the resulting subsets.\n",
        "\n",
        "-  n·µ¢: Number of instances in child i.\n",
        "\n",
        "-  n: Total number of instances in the parent node.\n",
        "\n",
        "-  Information Gain is used to select the best feature to split the data at each step while building the Decision Tree. The feature with the highest Information Gain is chosen because:\n",
        "\n",
        "-  It gives the most reduction in impurity.\n",
        "\n",
        "-  It results in purer child nodes, leading to a more accurate model.\n",
        "\n",
        "-  It helps the tree converge faster by reducing uncertainty more effectively.\n",
        "\n",
        "-  Example:\n",
        "  Suppose you‚Äôre building a decision tree to classify whether to play tennis based on weather conditions. If splitting by ‚ÄúOutlook‚Äù gives the highest Information Gain compared to ‚ÄúHumidity‚Äù or ‚ÄúWind‚Äù, then the tree will choose ‚ÄúOutlook‚Äù as the first split.  \n",
        "\n",
        "Question\n",
        "\n",
        "5: What are some common real-world applications of Decision Trees, and\n",
        "  what are their main advantages and limitations?\n",
        "-  answers:- Real-World Applications of Decision Trees:\n",
        "Medical Diagnosis\n",
        "\n",
        "-  Used to classify diseases based on patient symptoms, test results, and history.\n",
        "\n",
        "-  Example: Diagnosing diabetes or heart disease based on input features like age, blood pressure, glucose levels, etc.\n",
        "\n",
        "-  Customer Relationship Management (CRM)\n",
        "\n",
        "-  Predicting customer churn, segmenting customers, or recommending products.\n",
        "\n",
        "-  Credit Scoring and Risk Analysis\n",
        "\n",
        "-  Banks use decision trees to decide whether to approve loans based on financial history, income, credit score, etc.\n",
        "\n",
        "-  Fraud Detection\n",
        "\n",
        "-  Identifying fraudulent transactions by analyzing patterns in transaction data.\n",
        "\n",
        "- Marketing and Sales\n",
        "\n",
        "-  Targeting potential customers or selecting marketing strategies based on demographic and behavioral data.\n",
        "\n",
        "-  Manufacturing & Quality Control\n",
        "\n",
        "-  Predicting equipment failure or defects in a production process.\n",
        "\n",
        "-  Agriculture\n",
        "\n",
        "-  Classifying types of crops or predicting yield based on environmental factors like rainfall, temperature, and soil type.\n",
        "\n",
        "-  Main Advantages of Decision Trees:\n",
        "Easy to Understand and Interpret\n",
        "\n",
        "-  Tree structures are visual and mimic human decision-making.\n",
        "\n",
        "-  No Need for Feature Scaling or Normalization\n",
        "\n",
        "-  Can handle raw data without preprocessing like normalization or standardization.\n",
        "\n",
        "-  Can Handle Both Numerical and Categorical Data\n",
        "\n",
        "-  Flexibly deals with different types of variables.\n",
        "\n",
        "-  Non-Parametric\n",
        "\n",
        "-  Makes no assumptions about data distribution.\n",
        "\n",
        "-  Feature Selection is Built-in\n",
        "\n",
        "-  Automatically selects the most important features based on splitting criteria.\n",
        "\n",
        "-   Limitations of Decision Trees:\n",
        "Overfitting\n",
        "\n",
        "-  Trees can become too complex and memorize the training data instead of generalizing well (especially without pruning).\n",
        "\n",
        "-  Instability\n",
        "\n",
        "-  Small changes in the data can lead to very different tree structures.\n",
        "\n",
        "-  Biased Towards Features with More Levels\n",
        "\n",
        "-  Features with more unique values can dominate splits.\n",
        "\n",
        "-  Less Accurate Alone\n",
        "\n",
        "-  Compared to ensemble methods like Random Forest or Gradient Boosted Trees, a single decision tree may have lower predictive power.\n",
        "\n",
        "-  Harder to Model Complex Relationships\n",
        "\n",
        "-  Not as good at modeling interactions between variables unless the tree grows very deep.\n",
        "\n",
        "-   Summary Table:\n",
        "-   Aspect\tAdvantage or Limitation\tComment\n",
        "-  Interpretability\t‚úÖ Advantage\tEasy to explain and visualize\n",
        "-  Data Preprocessing\t‚úÖ Advantage\tNo need for normalization or scaling\n",
        "-  Overfitting Risk\t‚ùå Limitation\tNeeds pruning or ensemble methods\n",
        "-  Accuracy\t‚ùå Limitation\tLess accurate than boosted/ensemble models\n",
        "-  Stability\t‚ùå Limitation\tSensitive to small data changes\n",
        "\n",
        "*Dataset Info:\n",
        "‚óè Iris Dataset for classification tasks (sklearn.datasets.load_iris() or\n",
        "provided CSV).\n",
        "‚óè Boston Housing Dataset for regression tasks\n",
        "(sklearn.datasets.load_boston() or provided CSV).\n",
        "\n",
        "-  1. Iris Dataset (Classification)\n",
        "Purpose: Used for classification tasks ‚Äî predicting the species of an iris flower based on features.\n",
        "\n",
        "-   Features:\n",
        "sepal length (cm)\n",
        "\n",
        "sepal width (cm)\n",
        "\n",
        "petal length (cm)\n",
        "\n",
        "petal width (cm)\n",
        "\n",
        "-  Target:\n",
        "0: Setosa\n",
        "\n",
        "1: Versicolor\n",
        "\n",
        "2: Virginica\n",
        "\n",
        "-  Load using sklearn:\n",
        "from sklearn.datasets import load_iris\n",
        "import pandas as pd\n",
        "\n",
        "        iris = load_iris()\n",
        "        X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "        y = pd.Series(iris.target, name='species')\n",
        "\n",
        " 2. Boston Housing Dataset (Regression)\n",
        "Purpose: Used for regression tasks ‚Äî predicting median house prices in Boston suburbs.\n",
        "\n",
        "-   Note: load_boston() is deprecated due to ethical concerns with the dataset. If you're using an older version of scikit-learn, you can still load it as shown. For newer versions, use the CSV file or the California Housing dataset instead.\n",
        "\n",
        "-   Features:\n",
        "      13 numeric/categorical features (e.g., crime rate, number of rooms, property tax rate, etc.)\n",
        "\n",
        "-   Target:\n",
        "Median value of owner-occupied homes in $1000s.\n",
        "\n",
        "-   Load using sklearn (if available):\n",
        "from sklearn.datasets import load_boston\n",
        "import pandas as pd\n",
        "\n",
        "boston = load_boston()\n",
        "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
        "y = pd.Series(boston.target, name='MEDV')\n",
        "\n",
        "-   Alternative: Use CSV\n",
        "If load_boston() is not available, use the CSV (if provided):\n",
        "import pandas as pd\n",
        "\n",
        "-           df = pd.read_csv('boston_housing.csv')\n",
        "            X = df.drop('MEDV', axis=1)\n",
        "            y = df['MEDV']\n",
        "\n",
        "\n",
        "Question\n",
        "\n",
        "\n",
        " 6. Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier using the Gini criterion\n",
        "‚óè Print the model‚Äôs accuracy and feature importances\n",
        "\n",
        "-  answers :- Here is a complete Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "Train a Decision Tree Classifier using the Gini criterion\n",
        "Print the model‚Äôs accuracy and feature importances\n",
        "\n",
        "    # Import necessary libraries\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Load the Iris dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data  # Features\n",
        "    y = iris.target  # Labels\n",
        "\n",
        "    # Split the data into training and testing sets (80% train, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create and train the Decision Tree Classifier using Gini criterion\n",
        "    clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(\"Model Accuracy:\", accuracy)\n",
        "\n",
        "    # Print feature importances\n",
        "    print(\"\\nFeature Importances:\")\n",
        "    for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "    Model Accuracy: 1.0\n",
        "    Feature Importances:\n",
        "    sepal length (cm): 0.0000\n",
        "    sepal width (cm): 0.0000\n",
        "    petal length (cm): 0.4444\n",
        "    petal width (cm): 0.5556\n",
        "\n",
        "\n",
        "Question\n",
        "\n",
        "\n",
        "7:  Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree.\n",
        "-  answers:-\n",
        "-  Here's a complete Python program to:\n",
        "\n",
        "Load the Iris dataset\n",
        "\n",
        "Train two Decision Tree classifiers:\n",
        "\n",
        "One with max_depth=3\n",
        "\n",
        "Another fully grown (no max_depth limit)\n",
        "\n",
        "Compare their accuracies\n",
        "    # Import necessary libraries\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.metrics import accuracy_score\n",
        "\n",
        "    # Load the Iris dataset\n",
        "    iris = load_iris()\n",
        "    X = iris.data\n",
        "    y = iris.target\n",
        "\n",
        "    # Split the data into training and testing sets (80% train, 20% test)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    #  Train Decision Tree Classifier with max_depth = 3\n",
        "    clf_limited = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "    clf_limited.fit(X_train, y_train)\n",
        "    y_pred_limited = clf_limited.predict(X_test)\n",
        "    accuracy_limited = accuracy_score(y_test, y_pred_limited)\n",
        "\n",
        "    # Train fully-grown Decision Tree (no max_depth limit)\n",
        "    clf_full = DecisionTreeClassifier(random_state=42)\n",
        "    clf_full.fit(X_train, y_train)\n",
        "    y_pred_full = clf_full.predict(X_test)\n",
        "    accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "\n",
        "    # Output the results\n",
        "    print(\"Accuracy with max_depth=3:\", accuracy_limited)\n",
        "    print(\"Accuracy with fully-grown tree:\", accuracy_full)\n",
        "\n",
        "    Accuracy with max_depth=3: 1.0\n",
        "    Accuracy with fully-grown tree: 1.0\n",
        "\n",
        "Question\n",
        "\n",
        " 8: Write a Python program to:\n",
        "‚óè Load the Boston Housing Dataset\n",
        "‚óè Train a Decision Tree Regressor\n",
        "‚óè Print the Mean Squared Error (MSE) and feature importances\n",
        "-  answers:- Here's a Python program that:\n",
        "\n",
        "\n",
        "-  Note: load_boston was removed in scikit-learn v1.2 due to ethical concerns. You can still load it using a backup method or via sklearn.datasets.fetch_openml. Below is a version that works with current versions using OpenML:\n",
        "\n",
        "        from sklearn.datasets import fetch_openml\n",
        "        from sklearn.tree import DecisionTreeRegressor\n",
        "        from sklearn.metrics import mean_squared_error\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        import pandas as pd\n",
        "\n",
        "        # Load the Boston Housing dataset from OpenML\n",
        "         boston = fetch_openml(name='boston', version=1, as_frame=True)\n",
        "         X = boston.data\n",
        "        y = boston.target\n",
        "\n",
        "       # Split into training and testing sets (80% train, 20% test)\n",
        "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "       # Train Decision Tree Regressor\n",
        "      model = DecisionTreeRegressor(random_state=42)\n",
        "       model.fit(X_train, y_train)\n",
        "\n",
        "      # Predict on test set\n",
        "       y_pred = model.predict(X_test)\n",
        "\n",
        "      # Calculate Mean Squared Error (MSE)\n",
        "      mse = mean_squared_error(y_test, y_pred)\n",
        "      print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "\n",
        "       # Print feature importances\n",
        "      importances = pd.Series(model.feature_importances_, index=X.columns)\n",
        "      print(\"\\nFeature Importances:\")\n",
        "      print(importances.sort_values(ascending=False))\n",
        "\n",
        "      Mean Squared Error (MSE): 19.75\n",
        "\n",
        "      Feature Importances:\n",
        "      LSTAT    0.620\n",
        "\n",
        "      RM       0.275\n",
        "\n",
        "     DIS      0.033\n",
        "\n",
        "     CRIM     0.024\n",
        "\n",
        "     NOX      0.017\n",
        "\n",
        "     ...\n",
        "\n",
        "\n",
        "Question\n",
        "9: Write a Python program to:\n",
        "‚óè Load the Iris Dataset\n",
        "‚óè Tune the Decision Tree‚Äôs max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "‚óè Print the best parameters and the resulting model accuracy\n",
        "\n",
        "-  answers:- Here's a complete Python program to:\n",
        "\n",
        "        Print the best parameters and model accuracy\n",
        "\n",
        "        from sklearn.datasets import load_iris\n",
        "        from sklearn.tree import DecisionTreeClassifier\n",
        "        from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "        from sklearn.metrics import accuracy_score\n",
        "\n",
        "        # Load the Iris dataset\n",
        "        iris = load_iris()\n",
        "         y = iris.target\n",
        "\n",
        "        # Split the dataset into training and testing sets (80-20 split)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "        # Create a Decision Tree Classifier\n",
        "        dtree = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "        # Define the parameter grid to tune\n",
        "        param_grid = {\n",
        "        'max_depth': [2, 3, 4, 5, 6],\n",
        "        'min_samples_split': [2, 3, 4, 5]\n",
        "        }\n",
        "\n",
        "         # Apply GridSearchCV\n",
        "        grid_search = GridSearchCV(dtree, param_grid, cv=5, scoring='accuracy')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        # Get the best estimator\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # Predict on test data\n",
        "        y_pred = best_model.predict(X_test)\n",
        "\n",
        "        # Print the best parameters and accuracy\n",
        "        print(\"Best Parameters:\", grid_search.best_params_)\n",
        "        print(\"Test Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
        "\n",
        "        Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
        "        Test Accuracy: 100.00%\n",
        "\n",
        "Question\n",
        "\n",
        " 10: Imagine you‚Äôre working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with\n",
        "mixed data types and some missing values.\n",
        "Explain the step-by-step process you would follow to:\n",
        "‚óè Handle the missing values\n",
        "‚óè Encode the categorical features\n",
        "‚óè Train a Decision Tree model\n",
        "‚óè Tune its hyperparameters\n",
        "‚óè Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "-  answers:- Step 1: Handle the Missing Values\n",
        "Explore the data:\n",
        "\n",
        "Use .info() and .isnull().sum() to check for missing values.\n",
        "\n",
        "Impute missing values:\n",
        "\n",
        "Numerical columns: Use mean or median imputation depending on skewness.\n",
        "\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    num_imputer = SimpleImputer(strategy='median')\n",
        "    Categorical columns: Use mode (most frequent) imputation.\n",
        "\n",
        "-  cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "You can also use more advanced methods like KNN imputation for better accuracy.\n",
        "\n",
        "-  Step 2: Encode the Categorical Features\n",
        "Identify categorical columns.\n",
        "\n",
        "    Use df.select_dtypes(include='object') or df.dtypes.\n",
        "\n",
        "Encoding:\n",
        "\n",
        "For Decision Trees, Label Encoding is often sufficient, as trees are not sensitive to one-hot encoded scales.\n",
        "\n",
        "\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    le = LabelEncoder()\n",
        "    df['column'] = le.fit_transform(df['column'])\n",
        "    If the tree splits based on feature importance, OneHotEncoding can also be used:\n",
        "\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "-  Step 3: Train a Decision Tree Model\n",
        "Split the dataset:\n",
        "\n",
        "    from sklearn.model_selection import train_test_split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "     Train the model:\n",
        "\n",
        "    from sklearn.tree import DecisionTreeClassifier\n",
        "    dt = DecisionTreeClassifier(random_state=42)\n",
        "    dt.fit(X_train, y_train)\n",
        "\n",
        "-  Step 4: Tune Hyperparameters\n",
        "Use GridSearchCV or RandomizedSearchCV to tune:\n",
        "\n",
        "      max_depth, min_samples_split, min_samples_leaf, criterion (gini/entropy).\n",
        "\n",
        "\n",
        "      param_grid = {\n",
        "     'max_depth': [3, 5, 10, None],\n",
        "     'min_samples_split': [2, 5, 10],\n",
        "     'min_samples_leaf': [1, 2, 4],\n",
        "     'criterion': ['gini', 'entropy']\n",
        "      }\n",
        "\n",
        "     grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                            param_grid, cv=5, scoring='accuracy')\n",
        "     grid_search.fit(X_train, y_train)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "-  Step 5: Evaluate Performance\n",
        "Metrics to use:\n",
        "\n",
        "Accuracy\n",
        "\n",
        "Precision, Recall, F1-score\n",
        "\n",
        "Confusion Matrix\n",
        "\n",
        "ROC AUC Score\n",
        "\n",
        "\n",
        "    from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"ROC AUC:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:, 1]))\n",
        "    Business Value in Real-World Healthcare Setting\n",
        "    This model can provide the following key benefits:\n",
        "\n",
        "   Early Detection & Diagnosis: Helps doctors prioritize high-risk patients  for further tests, enabling faster intervention.\n",
        "\n",
        "Resource Optimization: Reduces unnecessary tests for low-risk patients, optimizing time and cost.\n",
        "\n",
        "Decision Support: Assists physicians in decision-making by identifying hidden patterns in patient data.\n",
        "\n",
        "Personalized Care: Enables better treatment planning based on predicted risk.\n",
        "\n",
        "Regulatory Reporting: Can be used to generate risk stratification for health insurance and compliance.\n",
        "\n",
        "Patient Outreach: Helps in targeted communication campaigns (e.g., invite high-risk groups for checkups).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5hvEJDwROLO4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f60U7ZR0Zt0i"
      }
    }
  ]
}